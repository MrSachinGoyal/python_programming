{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq5jjKHb-zdZ"
   },
   "source": [
    "# <u> Problem 1</u>\n",
    "\n",
    "### Write a function which takes the excel column name as an input and returns the corresponding column number. A few examples are :\n",
    "\n",
    "* column name = <code>'J'</code> , column number = <code>10</code>\n",
    "* column name = <code>'AP'</code> , column number = <code>42</code>\n",
    "* column name = <code>'AAA'</code>, column number = <code>703</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "Un0QXOa3-zdb"
   },
   "outputs": [],
   "source": [
    "def get_excel_column_number(column_name):\n",
    "    '''\n",
    "  This functions returns the corresponding column number for an excel column name\n",
    "    '''\n",
    "  # Write your code here\n",
    "    column_number = 0\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "VFSrFUwP-zdd"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "get_excel_column_number('BD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "AZaQlyAc-zde"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "get_excel_column_number('ALQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh_MA_BN-zd5"
   },
   "source": [
    "# <u> Problem 2</u>\n",
    "\n",
    "### We evaluate a standard machine learning classification model using various evaluation metrics. A classification model is a model which classifies a given observation or an event to a fixed set of categories. Suppose I train a machine learning model to classify images of cats and dogs. For each image, the machine classifies the image with either a <code>'Cat'</code> or a <code>'Dog'</code>. So in essence for each input image, there is a corresponding output by the model. This output can either be a <code>'Cat'</code> or a <code>'Dog'</code>.\n",
    "\n",
    "### To evaluate such a machine learning model which is trained to classify a given observation with at most two labels, a lot of candidate evaluation metrics are available. Accuracy is one of such evaluation metrics.\n",
    "\n",
    "### <u> Accuracy defintion </u> : Suppose you are given 20 input images of cats and dogs. You already know from these images that there are 11 cats and 9 dogs. You train a machine learning model to classify these images into cats and dogs. The machine predicts 9 cats correctly and 8 dogs correctly. The accuracy of the model is then defined as (correctly predicted cats and dogs)/(total cats and dogs). In this case it is (9+8)/(11+9) = 17/20 = 0.85 or 85%. It is a good practice to report the accuracy in percentages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp2SUazju_D7"
   },
   "source": [
    "### You are given two lists each of length 20 : one list contains the actual labels for the images of cats and dogs. The other list contains the predicted labels (by the machine) for the images of cats and dogs.\n",
    "\n",
    "### How to read the two lists ? For example for the first image, actual label is Cat and the predicted label is Cat. For the last image, actual label is Dog and the predicted label is Cat. The same index in the two lists corresponds to the same image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "BPsQbNPjvjAT"
   },
   "outputs": [],
   "source": [
    "actual_labels = ['Cat','Dog','Cat','Cat','Dog','Cat','Dog','Cat','Cat','Cat','Dog','Dog','Cat','Cat','Cat','Dog','Dog','Dog','Cat','Dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "MDefeYf1v4pT"
   },
   "outputs": [],
   "source": [
    "predicted_labels = ['Cat','Dog','Cat','Cat','Dog','Cat','Dog','Cat','Dog','Dog','Dog','Dog','Cat','Cat','Cat','Dog','Dog','Dog','Cat','Cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzLROuvWwZAL"
   },
   "source": [
    "### Write a function to calculate the accuracy. This functions takes two lists as inputs and returns the accuracy score in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "HvDJXfL3-zd6"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(actual, predicted):\n",
    "    '''\n",
    "    This functions calculates the accuracy based on two input lists\n",
    "    '''\n",
    "  # Write your code here\n",
    "    scores = [1 if actual == predicted else 0 for actual, predicted in list(zip(actual_labels, predicted_labels))]\n",
    "    accuracy = (sum(scores)*100)/len(actual_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "At0m0hcc-zd_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the classification model is 85.0%\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy score for the given lists\n",
    "result = calculate_accuracy(actual_labels, predicted_labels)\n",
    "print(f\"Accuracy score for the classification model is {result}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyYwXAlax36r"
   },
   "source": [
    "* ### <b><u>Precision</u></b> for cats is defined as the number of correctly predicted cats divided by the number of predicted cats. Report precision in percentages.\n",
    "\n",
    "* ### <b><u>Recall</u></b> for cats is defined as the number of correctly predicted cats divided by the actual number of cats. Report recall in percentages.\n",
    "\n",
    "### We can define the same two metrics for dogs as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwYIek-9yd4n"
   },
   "source": [
    "# <u> Problem 3 </u>\n",
    "\n",
    "### Write a Python function which returns the precision and recall for a given input label. Use the same two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "SojiZTMSyY4T"
   },
   "outputs": [],
   "source": [
    "def precision_recall(label = 'Dog'):\n",
    "    '''\n",
    "    This functions returns a tuple of precision and recall for a given input label\n",
    "    '''\n",
    "#   # Your code here\n",
    "    actual_label_count = actual_labels.count(label)\n",
    "    predicted_label_count = predicted_labels.count(label)\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    for actual, predicted in list(zip(actual_labels, predicted_labels)):\n",
    "        if actual == label and predicted == label:\n",
    "            correct_predictions += 1\n",
    "    precision = correct_predictions*100/predicted_label_count\n",
    "    recall = correct_predictions*100/actual_label_count\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 80.0% and Recall is 88.88888888888889%\n"
     ]
    }
   ],
   "source": [
    "precision, recall = precision_recall()\n",
    "print(f\"Precision is {precision}% and Recall is {recall}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-5eBWXa1Mla"
   },
   "source": [
    "# <u> Problem 4 </u>\n",
    "\n",
    "### Write a Python function which takes a sentence and a length value as inputs and returns the counts of those words from the sentence whose length is equal to the provided input length value.\n",
    "\n",
    "### Suppose if the input for the length value is 5, it will return the count of all those words which are of length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "9ueu9nJx1uUZ"
   },
   "outputs": [],
   "source": [
    "marvel_quote = \"The world has changed and none of us can go back. All we can do is our best, and sometimes the best that we can do is to start over.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "9OPAUOCT2VQW"
   },
   "outputs": [],
   "source": [
    "# Your function below. Make sure to remove the special characters such as full stop and comma.\n",
    "def fixed_length_word_counts(sentence, length=3):\n",
    "    '''\n",
    "    This function returns the count of the words with the given input length\n",
    "    '''\n",
    "    # Your code below\n",
    "    modified_sentence = sentence.replace(\",\", \"\").replace(\".\", \"\")\n",
    "    fixed_length_words = [word for word in modified_sentence.split() if len(word) == length]\n",
    "    \n",
    "    return len(fixed_length_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "Px7OQG0j2iab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check on the sample sentence\n",
    "fixed_length_word_counts(marvel_quote, length=3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
